{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando Pacotes Necessários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lit\n",
    "from minio import Minio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indicando variáveis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexão bem-sucedida. Buckets disponíveis:\n",
      "hive-metastore-trusted\n",
      "payments\n",
      "pessoas-e-organizacoes\n",
      "risk\n",
      "serasa-parser\n",
      "teste-felipe\n",
      "teste-joao\n",
      "teste-sync\n",
      "tests-schemas\n",
      "vini-teste\n"
     ]
    }
   ],
   "source": [
    "# # Variáveis de ambiente\n",
    "MINIO_ENDPOINT = \"seu_end_point\"\n",
    "MINIO_ACCESS_KEY = \"sua_acess_key\"\n",
    "MINIO_SECRET_KEY = \"seu_secret_key\"\n",
    "\n",
    "# Conexão Minio\n",
    "storage_options = {\n",
    "    \"AWS_ACCESS_KEY_ID\": \"sua_acess_key\",\n",
    "    \"AWS_SECRET_ACCESS_KEY\": \"seu_secret_key\",\n",
    "    \"AWS_ENDPOINT_URL\":\"seu_end_point\",\n",
    "    \"AWS_REGION\": \"us-east-1\",\n",
    "    \"AWS_S3_ALLOW_UNSAFE_RENAME\": \"true\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando sessão Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(# Dando um Nome para a instância \n",
    "        \"Delta_Spark\") \\\n",
    "        \\\n",
    "    .config(# Jars Necessários para utilizar os recursos\n",
    "        \"spark.jars\", \n",
    "        \"C:/Local/Onde/Esta/OSpark/jars/hadoop-aws-3.3.4.jar,\"\n",
    "        \"C:/Local/Onde/Esta/OSpark/jars/aws-java-sdk-bundle-1.12.732.jar,\"\n",
    "        \"C:/Local/Onde/Esta/OSpark/jars/delta-storage-3.2.0.jar,\"\n",
    "        \"C:/Local/Onde/Esta/OSpark/jars/delta-spark_2.12-3.2.0.jar\") \\\n",
    "        \\\n",
    "    .config(# Credenciais MiniO - MINIO_ACCESS_KEY\n",
    "        \"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY) \\\n",
    "        \\\n",
    "    .config(# Credenciais MiniO - MINIO_SECRET_KEY\n",
    "        \"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_KEY) \\\n",
    "        \\\n",
    "    .config(# Credenciais MiniO - MINIO_ENDPOINT\n",
    "        \"spark.hadoop.fs.s3a.endpoint\", MINIO_ENDPOINT) \\\n",
    "        \\\n",
    "    .config(# Ativa o acesso baseado em caminho (path-style) ao invés de virtual-hosted style. \n",
    "        \"fs.s3a.path.style.access\", \"true\") \\\n",
    "        \\\n",
    "    .config(# Define que o Spark deve usar a implementação do sistema de arquivos S3A para acessar o S3\n",
    "        \"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        \\\n",
    "    .config(# Usa a implementação de armazenamento de log S3SingleDriverLogStore para operações Delta Lake\n",
    "        \"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "        \\\n",
    "    .config(# Ativa extensões Delta Lake no Spark SQL, adicionando funcionalidades específicas do Delta Lake\n",
    "        \"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        \\\n",
    "    .config(# Define o catálogo Delta Lake como o catálogo SQL padrão do Spark, permitindo a criação e gestão de tabelas Delta Lake usando comandos SQL\n",
    "        \"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplo DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|first_name|age|\n",
      "+----------+---+\n",
      "|       bob| 47|\n",
      "|        li| 23|\n",
      "|   leonard| 51|\n",
      "+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Criando um DataFrame de Exemplo\n",
    "df = spark.createDataFrame([(\"joao\", 47), (\"maria\", 35), (\"pedro\", 51)]).toDF(\n",
    "    \"nome\", \"idade\"\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salvando em um Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame salvo como Delta Lake com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Salvando no MiniO\n",
    "try:\n",
    "    df.write \\\n",
    "      .option(\"mergeSchema\", \"true\") \\\n",
    "      .mode(\"append\") \\\n",
    "      .format(\"delta\") \\\n",
    "      .save(\"s3a://seu_caminho\", storage_options = storage_options)\n",
    "\n",
    "    print(\"DataFrame salvo com sucesso!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Erro ao salvar o DataFrame como Delta Lake:\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lendo o arquivo salvo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-------------+---------+\n",
      "|first_name|age|nacionalidade|     sexo|\n",
      "+----------+---+-------------+---------+\n",
      "|   leonard| 51|   brasileiro|masculino|\n",
      "|       bob| 47|   brasileiro|masculino|\n",
      "|        li| 23|   brasileiro|masculino|\n",
      "|   leonard| 51|   brasileiro|     NULL|\n",
      "|       bob| 47|   brasileiro|     NULL|\n",
      "|        li| 23|   brasileiro|     NULL|\n",
      "+----------+---+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(\"s3a://seu_caminho\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finalizando Sessão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encerrando Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
